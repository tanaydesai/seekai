import OpenAI from "openai";
import { OpenAIStream, StreamingTextResponse } from "ai";

const openai = new OpenAI({
  apiKey: process.env.NEXT_OPENAI_API_KEY,
});

export const runtime = "edge";

const createRequestMessages = async (req: Request) => {
  const { messages, data } = await req.json();
  if (!data?.imageUrl) return messages;

  const initialMessages = messages.slice(0, -1);
  const currentMessage = messages[messages.length - 1];
  return [
    ...initialMessages,
    {
      ...currentMessage,
      content: [
        { type: "text", text: currentMessage.content },
        {
          type: "image_url",
          image_url: data.imageUrl,
        },
      ],
    },
  ];
};

export async function POST(req: Request) {
  const inputMessages = await createRequestMessages(req);
  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    stream: true,
    messages: [...inputMessages, 
      {
      role: "system", 
      content: `You are a research paper assitant called Seek AI, you will provide answers to user's questions only based on the following paper content provided like summaries, citations, similar papers, deep dives etc. You were made by The Seek AI team consisting of Tanay, Jay, Pravir and Mitansh.`
      },
      {
      role: "user",
      content: ` 
          TinyStories: How Small Can Language Models Be and Still Speak
          Coherent English?
          Ronen Eldan∗ and Yuanzhi Li†
          Microsoft Research
          April 2023
          Abstract
          Language models[4, 5, 21] (LMs) are powerful tools for natural language processing, but they often struggle
          to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPTNeo (small) [3] or GPT-2 (small) [23] can rarely generate coherent and consistent English text beyond a few
          words even after extensive training. This raises the question of whether the emergence of the ability to produce
          coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex
          architectures (with many layers of global attention).
          In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a
          typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can
          be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million
          total parameters), or have much simpler architectures (with only one transformer block), yet still produce
          fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and
          demonstrate reasoning capabilities.
          We also introduce a new paradigm for the evaluation of language models: We suggest a framework which
          uses GPT-4 to grade the content generated by these models as if those were stories written by students and
          graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often
          require the model’s output to be very structured, and moreover it provides a multidimensional score for the
          model, providing scores for different capabilities such as grammar, creativity and instruction-following.
          We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for owresource or specialized domains, and shed light on the emergence of language capabilities in LMs.
      
          INTRODUCTION
          In this paper, we introduce TinyStories1
          , a synthetic dataset of short stories that are intended to contain only
          words that most 3 to 4-year-old children would typically understand, generated by GPT-3.5 and GPT-4. TinyStories is designed to capture the essence of natural language, while reducing its breadth and diversity. Each story
          consists of 2-3 paragraphs that follow a simple plot and a consistent theme, while the whole dataset aims to span
          the vocabulary and the factual knowledge base of a 3-4 year old child
          Based on this dataset, our paper makes several main contributions:
          • Our main contribution is that we show TinyStories can be used to train and evaluate SLMs2
          that are
          much smaller than the state-of-the-art models (below 10 million parameters with an embedding dimension
          of 256), or have much simpler architectures (with only one transformer block), yet still produce a diverse
          set of fluent and consistent stories that are comparable or superior to those generated by larger and
          more complex models. Moreover, despite of the small size of the models, we still observe an emergence of
          reasoning capabilities, knowledge of general facts and ability to follow certain instructions.
          • We introduce a new paradigm for evaluating language models using GPT-4, which overcomes many of the
          limitations of standard benchmarks.
          • We show that although the training of generative models on TinyStories can typically be done in less than a
          day on a single GPU, they still exhibit many behaviors similar to the ones observed in LLMs, such as scaling
          We show that the trained SLMs appear to be substantially more interpretable than larger ones. When
          models have a small number of neurons and/or a small number of layers, we observe that both attention
          heads and MLP neurons have a meaningful function: Attention heads produce very clear attention patterns,
          with a clear separation between local and semantic heads, and MLP neurons typically activated on tokens
          that have a clear common role in the sentence. We visualize and analyze the attention and activation maps
          of the models, and show how they relate to the generation process and the story content.
          To give the reader a first impression of the abilities of models trained on TinyStories, we compare the completion
          of a 28M parameter model trained on TinyStories3 with that of GPT2-XL, which is two orders of magnitude bigger
          (1.5B parameters), on a sample prompt4
          in Figure 1. We remark that the architectures and training scheme of the
          models are essentially the same.

          Conclusion
          In this work, we have presented TinyStories, a synthetic dataset of short stories that only contain words that a typical
          3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We have shown that TinyStories can be
          used to train and evaluate small language models (SLMs) that are much smaller than the state-of-the-art models,
          yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect
          grammar, and demonstrate reasoning capabilities.
          While large models trained on the huge and diverse language corpuses on the internet exhibit very impressive
          capabilities, those datasets appear to be too large for SLMs to capture the complex aspects of language. In this work
          we have argued that TinyStories enables us to observe and study the emergence of capabilities such as generation
          of coherent text, reasoning and instruction following in LMs on a much smaller scale, in terms of the size of both
          model and dataset. By training SLMs on our dataset, we have also observed many behaviors similar to LLMs such
          as scaling laws, trade-offs between width and depth, etc. Moreover, we have shown that the trained SLMs have
          much higher interpretability than larger ones, and that we can visualize and analyze their attention and activation
          patterns to understand how they generate and comprehend stories.

          REFRENCES
          [1] Common crawl. Accessed: 2019.
          [2] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation
          in deep learning. arXiv preprint arXiv:2012.09816, 2020.
          [3] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive
          Language Modeling with Mesh-Tensorflow, March 2021.
          [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
          Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
          Advances in neural information processing systems, 33:1877–1901, 2020.
          25
          [5] S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter
          Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Spark

          Similar papers:
          linear-algebra.pdf
          machine-learning.pdf
          backpropagation.pdf
          neural-networks.pdf
          adam.pdf
      `,
    }],
    max_tokens: 2000,
  });
  const stream = OpenAIStream(response);
  return new StreamingTextResponse(stream);
}
